{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### Approach 1 Base Line ###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import glob\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Select Baseline model (non-quantized)\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "baseline_model_dir = \"./models/baseline\"  # Baseline storing path\n",
    "print(f\"Model name (Baseline): {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Calculate download time**\n",
    "start_download = time.time()\n",
    "snapshot_path = snapshot_download(repo_id=model_name, cache_dir=baseline_model_dir)\n",
    "end_download = time.time()\n",
    "print(f\"Baseline model download time: {end_download - start_download:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Find model files**\n",
    "model_files = glob.glob(f\"{snapshot_path}/*\")\n",
    "print(f\"üìÇ Downloaded model files: {model_files}\")\n",
    "\n",
    "# **Calculate loading time**\n",
    "start_load = time.time()\n",
    "\n",
    "# ‚úÖ Use vLLM for inference, without AutoModelForCausalLM\n",
    "llm = LLM(model=snapshot_path, tensor_parallel_size=1)  # Single GPU\n",
    "tokenizer = AutoTokenizer.from_pretrained(snapshot_path)\n",
    "\n",
    "end_load = time.time()\n",
    "print(f\"Baseline model load time: {end_load - start_load:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Calculate inference time (Serve)**\n",
    "start_serve = time.time()\n",
    "\n",
    "prompt = \"Hello, how are you?\"\n",
    "sampling_params = SamplingParams(temperature=0.7, max_tokens=100)\n",
    "outputs = llm.generate([prompt], sampling_params)\n",
    "\n",
    "end_serve = time.time()\n",
    "print(f\"Serve (inference) time: {end_serve - start_serve:.2f} seconds\")\n",
    "\n",
    "print(f\"Generated response: {outputs[0].outputs[0].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### Approach 2 Quantized model ###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import glob\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"  # Use bnb-4bit model\n",
    "quantized_model_dir = \"./models/quantized\"  # Storing Path\n",
    "print(f\"Model name (Quantized): {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Calculate download time**\n",
    "start_download = time.time()\n",
    "snapshot_path = snapshot_download(repo_id=model_name, cache_dir=quantized_model_dir)\n",
    "end_download = time.time()\n",
    "print(f\"Quantized model download time: {end_download - start_download:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Find model files**\n",
    "model_files = glob.glob(f\"{snapshot_path}/*\")  \n",
    "print(f\"üìÇ Downloaded model files: {model_files}\")\n",
    "\n",
    "# **Calculate loading time**\n",
    "start_load = time.time()\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    snapshot_path,  \n",
    "    quantization_config=\"llama-4bit\",  # ‚úÖ  4-bit Quantization\n",
    "    device_map=\"auto\"  \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(snapshot_path)  # ‚úÖ Use tokenizer\n",
    "\n",
    "\n",
    "end_load = time.time()\n",
    "print(f\"Quantized model load time: {end_load - start_load:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Calculate inference time (Serve)**\n",
    "start_serve = time.time()\n",
    "\n",
    "prompt = \"Hello, how are you?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # ‚úÖ Infer on CUDA\n",
    "outputs = model.generate(**inputs)\n",
    "\n",
    "end_serve = time.time()\n",
    "print(f\"Serve (inference) time: {end_serve - start_serve:.2f} seconds\")\n",
    "\n",
    "print(f\"Generated response: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### Approach 3 Lazy Loading ###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üè∑Ô∏è Select model & set storage path\n",
    "model_name = \"meta-llama/llama-3.2-3b-instruct\"\n",
    "model_cache_dir = \"/content/models/llama-3.2-3b-instruct/\" \n",
    "\n",
    "# üìå Select the range of layers to load\n",
    "low_layer = 0\n",
    "high_layer = 15  # Only download the weights of the first 15 layers\n",
    "\n",
    "### **1Ô∏è‚É£ Hugging Face authentication (optional)**\n",
    "use_auth = False  \n",
    "if use_auth:\n",
    "    from huggingface_hub import login\n",
    "    huggingface_token = \"your_huggingface_token_here\"\n",
    "    login(token=huggingface_token)\n",
    "\n",
    "### **2Ô∏è‚É£ Create a cache directory**\n",
    "if not os.path.exists(model_cache_dir):\n",
    "    os.makedirs(model_cache_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### **3Ô∏è‚É£ Download only some weights**\n",
    "print(\"\\nüì• Downloading partial model weights...\")\n",
    "\n",
    "# Download model configuration and tokenizer\n",
    "hf_hub_download(repo_id=model_name, filename=\"config.json\", cache_dir=model_cache_dir)\n",
    "hf_hub_download(repo_id=model_name, filename=\"tokenizer.json\", cache_dir=model_cache_dir)\n",
    "\n",
    "# Download only weights from `low_layer` to `high_layer`\n",
    "start_download = time.perf_counter()\n",
    "for i in range(low_layer, high_layer):\n",
    "    filename = f\"model.layers.{i}.weight\"\n",
    "    hf_hub_download(repo_id=model_name, filename=filename, cache_dir=model_cache_dir)\n",
    "end_download = time.perf_counter()\n",
    "download_time = end_download - start_download\n",
    "print(f\"üì• Partial Model Download Time: {download_time:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### **4Ô∏è‚É£ Loading some models**\n",
    "print(f\"\\nüìÇ Loading only layers {low_layer} to {high_layer} ({high_layer - low_layer} layers)...\")\n",
    "start_load = time.perf_counter()\n",
    "\n",
    "config_path = os.path.join(model_cache_dir, \"config.json\")\n",
    "config = AutoConfig.from_pretrained(config_path)\n",
    "model = AutoModel.from_config(config)\n",
    "\n",
    "# Load only `N` layers of Transformer\n",
    "for i in range(low_layer, high_layer):\n",
    "    layer_path = os.path.join(model_cache_dir, f\"model.layers.{i}.weight\")\n",
    "    model.model.layers[i].load_state_dict(torch.load(layer_path))\n",
    "\n",
    "end_load = time.perf_counter()\n",
    "load_time = end_load - start_load\n",
    "print(f\"üìÇ Partial Model Loading Time: {load_time:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### **5Ô∏è‚É£ Redefine Partial Model**\n",
    "class PartialLlamaModel(torch.nn.Module):\n",
    "    def __init__(self, model, low_layer, high_layer):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = model.model.embed_tokens\n",
    "        self.layers = torch.nn.ModuleList(model.model.layers[low_layer:high_layer])\n",
    "        self.norm = model.model.norm\n",
    "        self.lm_head = model.lm_head\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, position_ids=None):\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(input_ids.shape[1], device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        hidden_states = self.embed_tokens(input_ids)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            query, key = hidden_states, hidden_states\n",
    "            cos, sin = apply_rotary_pos_emb(query, key, position_ids)\n",
    "            hidden_states = layer(hidden_states, attention_mask=attention_mask, position_embeddings=(cos, sin))[0]\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        return self.lm_head(hidden_states)\n",
    "\n",
    "# Create a Partial Model\n",
    "partial_model = PartialLlamaModel(model, low_layer, high_layer).to(\"cuda\")\n",
    "\n",
    "### **6Ô∏è‚É£ Reasoning Test**\n",
    "print(\"\\n‚ö° Running Inference with Partial Model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=model_cache_dir)\n",
    "\n",
    "prompt = \"Hello, how are you?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "start_infer = time.perf_counter()\n",
    "outputs = partial_model(**inputs)\n",
    "end_infer = time.perf_counter()\n",
    "inference_time = end_infer - start_infer\n",
    "print(f\"‚ö° Inference Time: {inference_time:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### **7Ô∏è‚É£ Output complete statistics**\n",
    "print(\"\\n=== üèÅ Timing Summary ===\")\n",
    "print(f\"üì• Partial Download Time: {download_time:.2f} sec\")\n",
    "print(f\"üìÇ Partial Load Time: {load_time:.2f} sec\")\n",
    "print(f\"‚ö° Inference Time: {inference_time:.2f} sec\")\n",
    "print(f\"üîπ Model: {model_name}\")\n",
    "print(f\"üîπ Loaded Layers: {low_layer} to {high_layer} ({high_layer - low_layer} layers)\")\n",
    "print(f\"üîπ Model Cache Directory: {model_cache_dir}\")\n",
    "print(f\"üîπ Approx. GPU Memory Usage: ~{(high_layer - low_layer) * 0.4:.1f} GB (Estimate)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
